# ğŸ„ Super Mario Bros RL Trainer

Ein Reinforcement Learning Projekt zum Trainieren eines KI-Agenten, der Super Mario Bros spielt. Das Projekt bietet eine benutzerfreundliche Web-OberflÃ¤che fÃ¼r Training, Visualisierung und Benchmarking verschiedener RL-Algorithmen.

## âœ¨ Features

- **Web-Interface**: Interaktive Streamlit-App fÃ¼r Training und Visualisierung
- **Mehrere RL-Algorithmen**: PPO, DQN, A2C mit optimierten Hyperparametern
- **Progress-Based Reward Shaping**: Intelligente Belohnungsfunktion fokussiert auf Level-Completion
- **Hardware-Optimierung**: Parallele Environments fÃ¼r schnelleres Training
- **Modell-Management**: Automatisches Speichern des besten Modells, Fortsetzen von Trainings
- **Live-Visualisierung**: Controller-Input-Anzeige wÃ¤hrend des Spielens
- **TensorBoard Integration**: Detaillierte Trainings-Metriken

## ğŸ“‹ Voraussetzungen

- Python 3.8 oder hÃ¶her
- macOS, Linux oder Windows
- FÃ¼r macOS: `ffmpeg` und `sdl2` (via Homebrew: `brew install ffmpeg sdl2`)

## ğŸš€ Installation

1. **Repository klonen oder herunterladen**

2. **Virtuelle Umgebung erstellen (empfohlen):**
   ```bash
   python -m venv venv
   source venv/bin/activate  # Windows: venv\Scripts\activate
   ```

3. **AbhÃ¤ngigkeiten installieren:**
   ```bash
   pip install -r requirements.txt
   ```

## ğŸ’» Nutzung

### Web-Interface (Empfohlen)

Starte die interaktive Web-App:

```bash
streamlit run app.py
```

Die App Ã¶ffnet sich automatisch im Browser unter `http://localhost:8501`.

#### Features im Web-Interface:

**Training Tab:**
- Algorithmus auswÃ¤hlen (PPO, DQN, A2C)
- Trainingsschritte konfigurieren
- Hardware-Optimierung (parallele Environments)
- Bestehendes Modell weiter trainieren
- Live-Fortschrittsanzeige

**Play / Watch Tab:**
- Agent beim Spielen zusehen
- Controller-Input in Echtzeit anzeigen
- Bestes Modell oder Standard-Modell wÃ¤hlen

### Terminal-Skripte

Alternativ kÃ¶nnen die Skripte direkt ausgefÃ¼hrt werden:

**Training:**
```bash
python train.py
```

**Testen:**
```bash
python test.py
```

## ğŸ¯ Reward-Shaping Strategie

Das Projekt verwendet eine **Progress-Based Reward Function**, die darauf ausgelegt ist, Level-Completion zu erreichen:

- **Progress Reward**: Belohnung nur fÃ¼r neuen Fortschritt (neue maximale X-Position)
- **Stagnation Penalty**: Strafe fÃ¼r langes Verharren ohne Fortschritt
- **Level Completion**: Massive Belohnung (+1000) fÃ¼r das Erreichen des Ziels
- **Death Penalty**: Strafe fÃ¼r Tod, aber ausgewogen, um Exploration zu ermÃ¶glichen

Diese Strategie verhindert "Reward Hacking" (z.B. Hin-und-Herlaufen) und zwingt den Agenten, echten Fortschritt zu machen.

## âš™ï¸ Konfiguration

### Trainingsparameter

- **Trainingsschritte**: Empfohlen 500.000+ fÃ¼r Level-Completion
- **Parallele Environments**: 1-8 (mehr = schneller, aber mehr RAM)
- **Subprocess-Modus**: Schneller, aber mehr RAM-Verbrauch

### Algorithmus-Empfehlungen

- **PPO**: Meist am besten fÃ¼r Super Mario Bros (empfohlen)
- **DQN**: Gut fÃ¼r Sample-Efficiency
- **A2C**: Schneller, aber oft weniger stabil

## ğŸ“Š TensorBoard

Um detaillierte Trainings-Metriken zu sehen:

```bash
tensorboard --logdir logs_web/
```

Ã–ffne dann `http://localhost:6006` im Browser.

## ğŸ“ Projektstruktur

```
SuperMario/
â”œâ”€â”€ app.py              # Streamlit Web-Interface
â”œâ”€â”€ train.py            # Terminal-Training-Skript
â”œâ”€â”€ test.py             # Terminal-Test-Skript
â”œâ”€â”€ requirements.txt    # Python-AbhÃ¤ngigkeiten
â”œâ”€â”€ README.md           # Diese Datei
â”œâ”€â”€ train_web/          # Gespeicherte Modelle
â””â”€â”€ logs_web/           # TensorBoard Logs
```

## ğŸ”§ Troubleshooting

### OverflowError: Python integer out of bounds for uint8

Dieser Fehler wurde bereits in den installierten Bibliotheken behoben. Falls er auftritt, starte den Streamlit-Server neu.

### Training dauert sehr lange

Das NES-Environment ist CPU-intensiv. Optimierungen:
- Mehr parallele Environments verwenden
- Subprocess-Modus aktivieren
- Frame-Skipping ist bereits auf 4 gesetzt (optimal)

### Agent lernt nicht / lÃ¤uft nur nach rechts

- Starte ein **neues Training** mit den aktuellen Reward-Funktionen
- Alte Modelle haben mÃ¶glicherweise "falsches" Verhalten gelernt
- Verwende mindestens 500.000 Trainingsschritte

## ğŸ“ Technische Details

### Environment Wrappers

- **SkipFrame**: Verarbeitet jeden 4. Frame (beschleunigt Training)
- **GrayScaleObservation**: Reduziert DimensionalitÃ¤t
- **ResizeObservation**: 84x84 Pixel (Standard fÃ¼r Deep RL)
- **VecFrameStack**: Stackt 4 Frames fÃ¼r Bewegungsinformation
- **RewardShaping**: Custom Reward-Funktion fÃ¼r Level-Completion

### Hyperparameter

Die Hyperparameter sind fÃ¼r Super Mario Bros optimiert:
- PPO: `learning_rate=2.5e-4`, `n_steps=2048`, `n_epochs=10`
- DQN: `buffer_size=100000`, `learning_starts=5000`
- A2C: `learning_rate=7e-4`, `n_steps=5`

## ğŸ“ Lizenz

Dieses Projekt verwendet:
- `gym-super-mario-bros` (MIT License)
- `stable-baselines3` (MIT License)
- `nes-py` (MIT License)

## ğŸ¤ Beitragen

Verbesserungen und Pull Requests sind willkommen!

## ğŸ“§ Support

Bei Problemen oder Fragen, Ã¶ffne ein Issue im Repository.

---

**Viel Erfolg beim Trainieren deines Super Mario Agenten! ğŸ„**
